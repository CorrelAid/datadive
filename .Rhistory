setwd("C:\\Users\\Friedrike\\Documents\\GitHub")
setwd("C:\\Users\\Friedrike\\Documents\\GitHub\\datadive")
#function to scrape information from individual pages
scrape_page <- function(html_file){
#load and parse html
html <- read_html(html_file, encoding = "UTF-8")
#id
id <- str_extract(html_file, "\\d{1,}")
#title
title <- html %>%
html_nodes(xpath = "//div[@class = 'col2']/descendant::h2") %>%
.[1] %>%
html_text(trim = T)
#who wrote petition
from <- html %>%
html_nodes(xpath = "//li[@class = 'petent']") %>%
html_text(trim = T)
#to whom it is directed
to <- html %>%
html_nodes(xpath = "//li[starts-with(./@class, 'empfaenger')]") %>%
html_text(trim = T)
#region
region <- html %>%
html_nodes(xpath = "//li[@class = 'region']/descendant::span") %>%
html_text(trim = T)
#category of petition
category <- html %>%
html_nodes(xpath = "//strong[contains(text(), 'Kategorie')]/parent::div") %>%
html_text(trim = T)
#total number of supporters
supporters_total <- html %>%
html_nodes(xpath = "//li[@class = 'unterstuetzer']/descendant::strong") %>%
html_text(trim = T) %>%
.[1]
#number of supporters in Germany
supporters_germany <- html %>%
html_nodes(xpath = "//li[@class = 'unterstuetzer']/descendant::strong") %>%
html_text(trim = T) %>%
.[2]
#percentage of target support acquired
perc_reached <- html %>%
html_nodes(xpath = "//div[@class = 'ziel percentage']") %>%
html_text(trim =  T)
#target support
target_support <- html %>%
html_nodes(xpath = "//div[@class = 'ziel']") %>%
html_text(trim =  T)
#status of petition
status <- html %>%
html_nodes(xpath = "//strong[contains(text(), 'Status')]/parent::div") %>%
html_text(trim = T)
#days left
days_left <- html %>%
html_nodes(xpath = "//li[@class = 'restzeit']/descendant::span") %>%
html_text(trim = T)
#petition text
petition_text <- html %>%
html_nodes(xpath = "//div[@class = 'col2']/descendant::div[@class = 'text']") %>%
html_text(trim =  T)
results <- cbind(id, title, from, to, region, category, status, target_support, perc_reached, supporters_total, supporters_germany, petition_text)
}
#load htmls of individual pages
htmls <-list.files(path="data/html_files", full.names = TRUE, recursive=FALSE)
#apply function to htmls
data_individualpages <- ldply(htmls, scrape_page)
rm(htmls)
#merge to petitions
petitions <- merge(petitions, data_individualpages, by = c("id"))
rm(data_individualpages)
#write
write.csv(petitions, "data/liste_in_zeichnung.csv")
library(rvest)
library(stringr)
library(plyr)
#function to scrape information from individual pages
scrape_page <- function(html_file){
#load and parse html
html <- read_html(html_file, encoding = "UTF-8")
#id
id <- str_extract(html_file, "\\d{1,}")
#title
title <- html %>%
html_nodes(xpath = "//div[@class = 'col2']/descendant::h2") %>%
.[1] %>%
html_text(trim = T)
#who wrote petition
from <- html %>%
html_nodes(xpath = "//li[@class = 'petent']") %>%
html_text(trim = T)
#to whom it is directed
to <- html %>%
html_nodes(xpath = "//li[starts-with(./@class, 'empfaenger')]") %>%
html_text(trim = T)
#region
region <- html %>%
html_nodes(xpath = "//li[@class = 'region']/descendant::span") %>%
html_text(trim = T)
#category of petition
category <- html %>%
html_nodes(xpath = "//strong[contains(text(), 'Kategorie')]/parent::div") %>%
html_text(trim = T)
#total number of supporters
supporters_total <- html %>%
html_nodes(xpath = "//li[@class = 'unterstuetzer']/descendant::strong") %>%
html_text(trim = T) %>%
.[1]
#number of supporters in Germany
supporters_germany <- html %>%
html_nodes(xpath = "//li[@class = 'unterstuetzer']/descendant::strong") %>%
html_text(trim = T) %>%
.[2]
#percentage of target support acquired
perc_reached <- html %>%
html_nodes(xpath = "//div[@class = 'ziel percentage']") %>%
html_text(trim =  T)
#target support
target_support <- html %>%
html_nodes(xpath = "//div[@class = 'ziel']") %>%
html_text(trim =  T)
#status of petition
status <- html %>%
html_nodes(xpath = "//strong[contains(text(), 'Status')]/parent::div") %>%
html_text(trim = T)
#days left
days_left <- html %>%
html_nodes(xpath = "//li[@class = 'restzeit']/descendant::span") %>%
html_text(trim = T)
#petition text
petition_text <- html %>%
html_nodes(xpath = "//div[@class = 'col2']/descendant::div[@class = 'text']") %>%
html_text(trim =  T)
results <- cbind(id, title, from, to, region, category, status, target_support, perc_reached, supporters_total, supporters_germany, petition_text)
}
#load htmls of individual pages
htmls <-list.files(path="data/html_files", full.names = TRUE, recursive=FALSE)
#apply function to htmls
data_individualpages <- ldply(htmls, scrape_page)
rm(htmls)
#merge to petitions
petitions <- merge(petitions, data_individualpages, by = c("id"))
rm(data_individualpages)
#write
write.csv(petitions, "data/liste_in_zeichnung.csv")
petitions <- read.csv("liste_in_zeichnung.csv", header = T, stringsAsFactors = F)
petitions <- read.csv("data/liste_in_zeichnung.csv", header = T, stringsAsFactors = F)
petitions <- select(petitions, X, names, urls)
library(dplyr)
petitions <- select(petitions, X, names, urls)
write.csv(petitions, "data/liste_in_zeichnung.csv")
#load htmls of individual pages
htmls <-list.files(path="data/html_files", full.names = TRUE, recursive=FALSE)
#apply function to htmls
data_individualpages <- ldply(htmls, scrape_page)
rm(htmls)
#merge to petitions
petitions <- merge(petitions, data_individualpages, by = c("id"))
rm(data_individualpages)
#apply function to htmls
data_individualpages <- ldply(htmls, scrape_page)
rm(htmls)
htmls <-list.files(path="data/html_files", full.names = TRUE, recursive=FALSE)
#apply function to htmls
data_individualpages <- ldply(htmls, scrape_page)
rm(htmls)
View(petitions)
#load petitions
petitions <- read.csv("data/liste_in_zeichnung.csv", header = T, stringsAsFactors = F)
#create new directory
dir.create("data/html_files")
#download html files, create id variable
for(i in 1:length(petitions$urls)){
content <- getURL(petitions$urls[i])
write(content, str_c("data/html_files/", i, ".html"))
petitions$id[i] <- i
Sys.sleep(0.2)
}
write.csv(petitions, "data/liste_in_zeichnung.csv")
View(petitions)
# Loading the R packages neccessary to execute the tasks ahead.
packages <- c("stringr", "XML", "maptools", "RCurl", "ggplot2", "sp", "spdep",
"rgdal", "reshape")
for (p in packages) {
if (p %in% installed.packages()[,1]) require(p, character.only=T)
else {
install.packages(p)
require(p, character.only=T)
}
}
URL <- 'https://www.openpetition.de/liste/in_zeichnung' # URL zur ersten Seite
file <- 'data/listen/liste_in_zeichnung1.html' # Pfad wo HTML-Seite gespeichert
# wird
download.file(URL, file)  # Datei wird heruntergeladen
parsed_doc <- htmlParse(file)  # Einlesen des HTML Codes
# Diesr Code liest den Text des Links zur letzten Seiten ein
# So wird identifiziert wieviele Seiten die Liste der Petitionen umfasst
num_pages <-
xpathSApply(parsed_doc, "//p[@class = 'pager']/a[last()]", fun = xmlValue)
num_pages <- as.integer(num_pages)
# ------------------------------------------------------------------------------
# Alle Seiten herunterladen
# ------------------------------------------------------------------------------
# Dieser ruft einzeln https://www.openpetition.de/liste/in_zeichnung?seite=*p*
# auf und speichert die vom Server generierte HTML-Seite
# *p* ist die Seitenzahl
# Die Schleife läuft bis die max. Seitenzahl (aus dem vorherigen Schritt)
# erreicht ist
for(p in 2:num_pages) {
download.file(paste0(URL, '?zeichnung=', p),
paste0('data/listen/liste_in_zeichnung_', p, '.html'))
}
# ------------------------------------------------------------------------------
# Liste aus Seite auslesen
# ------------------------------------------------------------------------------
# Leerer data.frame der die Liste der Petitionen von allen Seiten enthalten wird
petitionen <- data.frame()
# Die Loop durchläuft alle HTML-Seiten der Liste und liest die Petitionen aus
# und fügt diese zu einer Liste zusammen
for(p in 1:num_pages) {
# Datei einlesen -------------------------------------------------------------
file <- paste0('data/listen/liste_in_zeichnung_', p, '.html')
parsed_doc <- htmlParse(file)
# Datei auslesen -------------------------------------------------------------
# Liest die Titel der Petionen aus der Liste aus
name <- xpathSApply(parsed_doc, "//ul[@class = 'petitionen-liste']//h2/a",
fun = xmlValue)
# name säubern
name <- gsub('\n|\t|\t ', '', names)
# Liest die Links zu den Petitionen aus der Liste aus
url <- xpathSApply(parsed_doc, "//ul[@class = 'petitionen-liste']//h2/a",
xmlGetAttr, 'href')
# URLs enthalten nur relativen Pfad, Domain ergänzen für absoluten Pfad
url <- paste0('https://www.openpetition.de', urls)
tmp <- data.frame(name, url)
# Neue Seiten an Liste der Petitionen anhängen
petitionen <- rbind(petitionen, tmp)
}
write.csv(petitionen, 'data/liste_in_zeichnung.csv')
URL <- 'https://www.openpetition.de/liste/in_zeichnung' # URL zur ersten Seite
file <- 'data/listen/liste_in_zeichnung1.html' # Pfad wo HTML-Seite gespeichert
# wird
download.file(URL, file)  # Datei wird heruntergeladen
parsed_doc <- htmlParse(file)  # Einlesen des HTML Codes
# Diesr Code liest den Text des Links zur letzten Seiten ein
# So wird identifiziert wieviele Seiten die Liste der Petitionen umfasst
num_pages <-
xpathSApply(parsed_doc, "//p[@class = 'pager']/a[last()]", fun = xmlValue)
num_pages <- as.integer(num_pages)
for(p in 2:num_pages) {
download.file(paste0(URL, '?zeichnung=', p),
paste0('data/listen/liste_in_zeichnung_', p, '.html'))
}
petitionen <- data.frame()
for(p in 1:num_pages) {
# Datei einlesen -------------------------------------------------------------
file <- paste0('data/listen/liste_in_zeichnung_', p, '.html')
parsed_doc <- htmlParse(file)
# Datei auslesen -------------------------------------------------------------
# Liest die Titel der Petionen aus der Liste aus
name <- xpathSApply(parsed_doc, "//ul[@class = 'petitionen-liste']//h2/a",
fun = xmlValue)
# name säubern
name <- gsub('\n|\t|\t ', '', names)
# Liest die Links zu den Petitionen aus der Liste aus
url <- xpathSApply(parsed_doc, "//ul[@class = 'petitionen-liste']//h2/a",
xmlGetAttr, 'href')
# URLs enthalten nur relativen Pfad, Domain ergänzen für absoluten Pfad
url <- paste0('https://www.openpetition.de', urls)
tmp <- data.frame(name, url)
# Neue Seiten an Liste der Petitionen anhängen
petitionen <- rbind(petitionen, tmp)
}
for(p in 1:num_pages) {
# Datei einlesen -------------------------------------------------------------
file <- paste0('data/listen/liste_in_zeichnung_', as.character(p), '.html')
parsed_doc <- htmlParse(file)
# Datei auslesen -------------------------------------------------------------
# Liest die Titel der Petionen aus der Liste aus
name <- xpathSApply(parsed_doc, "//ul[@class = 'petitionen-liste']//h2/a",
fun = xmlValue)
# name säubern
name <- gsub('\n|\t|\t ', '', name)
# Liest die Links zu den Petitionen aus der Liste aus
url <- xpathSApply(parsed_doc, "//ul[@class = 'petitionen-liste']//h2/a",
xmlGetAttr, 'href')
# URLs enthalten nur relativen Pfad, Domain ergänzen für absoluten Pfad
url <- paste0('https://www.openpetition.de', url)
tmp <- data.frame(name, url)
# Neue Seiten an Liste der Petitionen anhängen
petitionen <- rbind(petitionen, tmp)
}
write.csv(petitionen, 'data/liste_in_zeichnung.csv')
#load petitions
petitions <- read.csv("data/liste_in_zeichnung.csv", header = T, stringsAsFactors = F)
#create new directory
dir.create("data/html_files")
#download html files, create id variable
for(i in 1:length(petitions$urls)){
content <- getURL(petitions$urls[i])
write(content, str_c("data/html_files/", i, ".html"))
petitions$id[i] <- i
Sys.sleep(0.2)
}
for(i in 1:length(petitions$urls)){
content <- getURL(petitions$urls[i])
write(content, str_c("data/html_files/", i, ".html"))
petitions$id[i] <- i
Sys.sleep(0.2)
}
write.csv(petitions, "data/liste_in_zeichnung.csv")
View(petitionen)
for(i in 1:length(petitions$urls)){
content <- getURL(petitions$urls[i])
write(content, str_c("data/html_files/", i, ".html"))
petitions$id[i] <- i
Sys.sleep(0.2)
}
write.csv(petitions, "data/liste_in_zeichnung.csv")
petitions[1]
#download html files, create id variable
for(i in 1:length(petitions$url)){
content <- getURL(petitions$url[i])
write(content, str_c("data/html_files/", i, ".html"))
petitions$id[i] <- i
Sys.sleep(0.2)
}
write.csv(petitions, "data/liste_in_zeichnung_withid.csv")
petitions <- read.csv("data/liste_in_zeichnung_withid.csv", header = T, stringsAsFactors = F)
#load htmls of individual pages
htmls <-list.files(path="data/html_files", full.names = TRUE, recursive=FALSE)
#apply function to htmls
data_individualpages <- ldply(htmls, scrape_page)
rm(htmls)
View(petitions)
write.csv(petitionen, 'data/1_liste_in_zeichnung.csv', row_names = F)
write.csv(petitionen, 'data/1_liste_in_zeichnung.csv', row.names = F)
petitions <- read.csv("data/1_liste_in_zeichnung.csv", header = T, stringsAsFactors = F)
#download html files, create id variable
for(i in 1:length(petitions$url)){
content <- getURL(petitions$url[i])
write(content, str_c("data/html_files/", i, ".html"))
petitions$id[i] <- i
Sys.sleep(0.2)
}
write.csv(petitions, "data/2_liste_in_zeichnung_withid.csv")
petitions <- read.csv("data/2_liste_in_zeichnung_withid.csv", header = T, stringsAsFactors = F)
petitions <- select(petitions, -X)
View(petitions)
write.csv(petitions, "data/2_liste_in_zeichnung_withid.csv", row.names = F)
petitions <- read.csv("data/2_liste_in_zeichnung_withid.csv", header = T, stringsAsFactors = F)
htmls <-list.files(path="data/html_files", full.names = TRUE, recursive=FALSE)
data_individualpages <- ldply(htmls, scrape_page)
rm(htmls)
petitions <- merge(petitions, data_individualpages, by = c("id"))
rm(data_individualpages)
write.csv(petitions, "data/3_liste_in_zeichnung_scraped.csv", row.names = F)
petitions <- read.csv("data/3_liste_in_zeichnung_scraped.csv", header = T, stringsAsFactors = F)
#inspect dataset for types etc. / which variables could be numeric, but are still character?
str(petitions)
for(i in 1:ncol(petitions)){
if(is.character(petitions[, i])){
Encoding(petitions[, i]) <- "UTF-8"
}else{
}}
str(petitions)
petitions <- petitions %>%
select(-name)
petitions <- petitions %>%
mutate(from = str_replace(from, "Von: (.+)", "\\1"))
petitions <- petitions %>%
mutate(target_support = str_replace_all(target_support, "\\n", "")) %>%
mutate(target_support = str_replace_all(target_support, "\\t", "")) %>%
mutate(target_support = str_replace_all(target_support, "(.+?) "))
library(stringr)
petitions <- petitions %>%
mutate(target_support = str_replace_all(target_support, "\\n", "")) %>%
mutate(target_support = str_replace_all(target_support, "\\t", "")) %>%
mutate(target_support = str_replace_all(target_support, "(.+?) "))
petitions <- petitions %>%
mutate(target_support = str_replace_all(target_support, "\\n", "")) %>%
mutate(target_support = str_replace_all(target_support, "\\t", "")) %>%
mutate(target_support = str_replace_all(target_support, "\\r", ""))
petitions <- petitions %>%
mutate(petition_text = str_replace_all(petition_text, "\\n", "")) %>%
mutate(petition_text = str_replace_all(petition_text, "\\t", ""))
View(petitions)
petitions <- petitions %>%
mutate(target_support = str_replace_all(target_support, "\\n", "")) %>%
mutate(target_support = str_replace_all(target_support, "\\t", "")) %>%
mutate(target_support = str_replace_all(target_support, "(\\d{1,}\\.{0,}\\d{1,}).+?", "\\1"))
View(petitions)
petitions <- petitions %>%
mutate(target_support = str_replace_all(target_support, "\\n", "")) %>%
mutate(target_support = str_replace_all(target_support, "\\t", "")) %>%
mutate(target_support = str_replace_all(target_support, "(\\d{1,}\\.{1,}\\d{1,}).+?", "\\1"))
str(petitions$target_support)
petitions <- petitions %>%
mutate(target_support = str_replace_all(target_support, "\\n", "")) %>%
mutate(target_support = str_replace_all(target_support, "\\t", "")) %>%
mutate(target_support = str_replace_all(target_support, "(\\d{1,}\\.{1,}\\d{1,})für.+?", "\\1"))
str(petitions$target_support)
petitions <- petitions %>%
mutate(target_support = str_replace_all(target_support, "\\n", "")) %>%
mutate(target_support = str_replace_all(target_support, "\\t", "")) %>%
mutate(target_support = str_replace_all(target_support, "(\\d{1,}\\.{1,}\\d{1,}).+", "\\1"))
str(petitions$target_support)
#mofidy target_support variable
petitions <- petitions %>%
mutate(target_support = str_replace_all(target_support, "\\n", "")) %>%
mutate(target_support = str_replace_all(target_support, "\\t", "")) %>%
mutate(target_support = as.numeric(str_replace_all(target_support, "(\\d{1,}\\.{1,}\\d{1,}).+", "\\1")))
str(petitions$target_support)
#load dataset
petitions <- read.csv("data/3_liste_in_zeichnung_scraped.csv", header = T, stringsAsFactors = F)
#inspect dataset for types etc. / which variables could be numeric, but are still character?
str(petitions)
#Encoding
for(i in 1:ncol(petitions)){
if(is.character(petitions[, i])){
Encoding(petitions[, i]) <- "UTF-8"
}else{
}}
rm(i)
#remove name variable
petitions <- petitions %>%
select(-name)
#modify from variable
petitions <- petitions %>%
mutate(from = str_replace(from, "Von: (.+)", "\\1"))
#mofidy target_support variable
petitions <- petitions %>%
mutate(target_support = str_replace_all(target_support, "\\n", "")) %>%
mutate(target_support = str_replace_all(target_support, "\\t", "")) %>%
mutate(target_support = str_replace_all(target_support, "(\\d{1,}\\.{1,}\\d{1,}).+", "\\1")) %>%
mutate(target_support = str_replace_all(target_support, "\\.", "")) %>%
mutate(target_support = as.numeric(target_support))
str(petitions$target_support)
write.csv(petitions, "data/4_liste_in_zeichnung_clean.csv", row.names = F)
